{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1KbYkGl75RqYn_h0sqyHIjZFNVW8DyFTq","authorship_tag":"ABX9TyPUzki50rn/Cm7DIVMmEHIb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# pip install scikeras"],"metadata":{"id":"HflXrWFf8cRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8L8phsBH2VqF"},"outputs":[],"source":["'''\n"," -----------------------------------------------------------\n","          Artificial Intelligence Workshop RUG\n"," -----------------------------------------------------------\n","            R.M. (Rolando) Gonzales Martinez\n"," -----------------------------------------------------------\n"," ~ ~ ~ ~ ~ ~ ~  Population forecasts with AI  ~ ~ ~ ~ ~ ~ ~\n","    Small area population forecasts with LSTM & GRU\n","   models and hyper-parameter fine-tuning with grid search\n"," -----------------------------------------------------------\n","'''\n","import pandas as pd\n","import os, random\n","import numpy as np\n","import tensorflow as tf\n","\n","os.environ['PYTHONHASHSEED'] = '0'\n","random.seed(0)\n","np.random.seed(0)\n","tf.random.set_seed(0)\n","\n","# 1. Loading data\n","url = \"https://raw.githubusercontent.com/rogon666/AI_workshop/refs/heads/main/02_databases/Berlinpopulation.csv\"\n","df = pd.read_csv(url)"]},{"cell_type":"code","source":["# Grid search hypertuning\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from itertools import product\n","from tqdm import tqdm\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.metrics import (\n","    mean_squared_error,\n","    mean_absolute_error,\n","    mean_absolute_percentage_error\n",")\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Input, Dense, LSTM, GRU\n","\n","\n","# 2. TRAIN/TEST SPLIT\n","TEST_SIZE = 36\n","train_df = df.iloc[:-TEST_SIZE].reset_index(drop=True)\n","test_df  = df.iloc[-TEST_SIZE:].reset_index(drop=True)\n","\n","# 3. SCALE POPULATION\n","scaler = MinMaxScaler()\n","train_scaled = scaler.fit_transform(train_df['population'].values.reshape(-1,1))\n","\n","# 4. MAKE SEQUENCES\n","SEQ_LEN = 10\n","def make_sequences(series, seq_len):\n","    X, y = [], []\n","    for i in range(seq_len, len(series)):\n","        X.append(series[i-seq_len:i, 0])\n","        y.append(series[i, 0])\n","    return np.array(X).reshape(-1, seq_len, 1), np.array(y)\n","\n","x_train_seq, y_train_seq = make_sequences(train_scaled, SEQ_LEN)\n","\n","# 5. GRID SEARCH w/ ONE OVERALL PROGRESS BAR\n","param_grid = {\n","    'cell_type':  ['LSTM', 'GRU'],\n","    'units':      [20, 50, 100],\n","    'batch_size': [1, 5],\n","    'epochs':     [5, ] # <--------------------------- fill here\n","}\n","param_list = list(product(\n","    param_grid['cell_type'],\n","    param_grid['units'],\n","    param_grid['batch_size'],\n","    param_grid['epochs']\n","))\n","tscv = TimeSeriesSplit(n_splits=3)\n","\n","# track best for each cell type\n","best_score = {'LSTM': np.inf, 'GRU': np.inf}\n","best_params = {'LSTM': None,   'GRU': None}\n","\n","print(\"Starting grid search:\")\n","for cell_type, units, batch_size, epochs in tqdm(\n","    param_list,\n","    desc=\"Overall grid search\",\n","    unit=\"config\",\n","    ncols=80\n","):\n","    cv_scores = []\n","    for train_idx, val_idx in tscv.split(x_train_seq):\n","        X_tr, X_val = x_train_seq[train_idx], x_train_seq[val_idx]\n","        y_tr, y_val = y_train_seq[train_idx], y_train_seq[val_idx]\n","\n","        # build model with Input layer\n","        model = Sequential([\n","            Input(shape=(SEQ_LEN,1)),\n","            (LSTM(units, return_sequences=True) if cell_type=='LSTM'\n","             else GRU(units, return_sequences=True)),\n","            (LSTM(units) if cell_type=='LSTM'\n","             else GRU(units)),\n","            Dense(1)\n","        ])\n","        model.compile(loss='mean_squared_error', optimizer='adam')\n","\n","        # train silently\n","        model.fit(\n","            X_tr, y_tr,\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            verbose=0,\n","            shuffle=False\n","        )\n","\n","        preds = model.predict(X_val, verbose=0).flatten()\n","        cv_scores.append(mean_squared_error(y_val, preds))\n","\n","    mean_cv = np.mean(cv_scores)\n","    if mean_cv < best_score[cell_type]:\n","        best_score[cell_type] = mean_cv\n","        best_params[cell_type] = {\n","            'units':      units,\n","            'batch_size': batch_size,\n","            'epochs':     epochs\n","        }\n","\n","# report best for each\n","print(\"\\nBest hyperparameters per model:\")\n","for ct in ['LSTM','GRU']:\n","    bp = best_params[ct]\n","    print(f\"{ct}: units={bp['units']}, batch_size={bp['batch_size']}, \"\n","          f\"epochs={bp['epochs']} â†’ CV MSE={best_score[ct]:.4f}\")\n","\n","# 6. RE-TRAIN & FORECAST BOTH MODELS\n","# prepare test sequences\n","full_scaled = scaler.transform(df['population'].values.reshape(-1,1))[:,0]\n","inputs = full_scaled[-(TEST_SIZE + SEQ_LEN):]\n","x_test_seq = np.array([inputs[i-SEQ_LEN:i] for i in range(SEQ_LEN, len(inputs))])\n","x_test_seq = x_test_seq.reshape(-1, SEQ_LEN, 1)\n","y_test = test_df['population'].values\n","years_test = test_df['year'].values\n","\n","predictions = {}\n","models = {}\n","\n","for cell_type in ['LSTM','GRU']:\n","    params = best_params[cell_type]\n","    Cell = LSTM if cell_type=='LSTM' else GRU\n","\n","    # build & train on full train set\n","    model = Sequential([\n","        Input(shape=(SEQ_LEN,1)),\n","        Cell(params['units'], return_sequences=True),\n","        Cell(params['units']),\n","        Dense(1)\n","    ])\n","    model.compile(loss='mean_squared_error', optimizer='adam')\n","    print(f\"\\nTraining best {cell_type} model:\")\n","    model.fit(\n","        x_train_seq, y_train_seq,\n","        epochs=params['epochs'],\n","        batch_size=params['batch_size'],\n","        verbose=1,\n","        shuffle=False\n","    )\n","\n","    # forecast\n","    pred_scaled = model.predict(x_test_seq, verbose=0)\n","    pred = scaler.inverse_transform(pred_scaled.reshape(-1,1)).flatten()\n","    predictions[cell_type] = pred\n","    models[cell_type] = model\n","\n","# 7. METRICS & COMPARISON TABLE\n","def rmse(y_true, y_pred):\n","    return np.sqrt(mean_squared_error(y_true, y_pred))\n","\n","metrics = []\n","for ct in ['LSTM','GRU']:\n","    y_pred = predictions[ct]\n","    metrics.append({\n","        'Model':  ct,\n","        'MSE':    mean_squared_error(y_test, y_pred),\n","        'RMSE':   rmse(y_test, y_pred),\n","        'MAE':    mean_absolute_error(y_test, y_pred),\n","        'MAPE (%)': mean_absolute_percentage_error(y_test, y_pred)*100,\n","    })\n","\n","metrics_df = pd.DataFrame(metrics).set_index('Model').round(4)\n","print(\"\\nTest-set performance:\")\n","print(metrics_df)\n","\n","# comparison table\n","comp_df = pd.DataFrame({\n","    'Year':          years_test,\n","    'Actual':        y_test,\n","    'LSTM Forecast': predictions['LSTM'],\n","    'GRU Forecast':  predictions['GRU']\n","})\n","print(\"\\nForecast comparison:\")\n","print(comp_df.to_markdown(index=False))\n","\n","# =======================\n","# 8. PLOT TEST FORECASTS\n","# =======================\n","plt.figure(figsize=(8,5))\n","plt.plot(years_test, y_test,               label='Actual',        color='black', linewidth=2)\n","plt.plot(years_test, predictions['LSTM'],  label='LSTM Forecast', linestyle='--', linewidth=2)\n","plt.plot(years_test, predictions['GRU'],   label='GRU Forecast',  linestyle=':',  linewidth=2)\n","plt.title(\"Test-Sample: Actual vs. LSTM & GRU Forecasts\")\n","plt.xlabel(\"Year\"); plt.ylabel(\"Population\")\n","plt.legend(); plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"ikxb8Kw6ETQq"},"execution_count":null,"outputs":[]}]}